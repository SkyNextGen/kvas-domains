#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KVAS domains build reporter

Inputs:
  - dist/state.json (produced by build pipeline)

Outputs:
  - dist/report.md
  - dist/tg_message.txt   (Telegram message in approved format: ĞĞš/ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•/ĞĞ¨Ğ˜Ğ‘ĞšĞ)
  - dist/tg_alert.txt     (Only for ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•/ĞĞ¨Ğ˜Ğ‘ĞšĞ; optional)
  - dist/stats.json       (rolling telemetry for trend)

This file is meant to be committed as scripts/report.py (or similar).
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple


# ------------------------- paths -------------------------

ROOT = Path(__file__).resolve().parents[1]
DIST_DIR = ROOT / "dist"

STATE_JSON = DIST_DIR / "state.json"
REPORT_OUT = DIST_DIR / "report.md"
TG_MESSAGE_OUT = DIST_DIR / "tg_message.txt"
TG_ALERT_OUT = DIST_DIR / "tg_alert.txt"
STATS_JSON = DIST_DIR / "stats.json"


# ------------------------- time/locale -------------------------

MSK_TZ = timezone(timedelta(hours=3))
MONTHS_RU = ["ÑĞ½Ğ²", "Ñ„ĞµĞ²", "Ğ¼Ğ°Ñ€", "Ğ°Ğ¿Ñ€", "Ğ¼Ğ°Ñ", "Ğ¸ÑĞ½", "Ğ¸ÑĞ»", "Ğ°Ğ²Ğ³", "ÑĞµĞ½", "Ğ¾ĞºÑ‚", "Ğ½Ğ¾Ñ", "Ğ´ĞµĞº"]


def now_msk_dt() -> datetime:
    return datetime.now(timezone.utc).astimezone(MSK_TZ)


def format_build_time_msk_from_state(build_time_utc_raw: str) -> str:
    """
    state.json typically stores build_time_utc like:
      - 'YYYY-MM-DD HH:MM:SS UTC'
      - 'YYYY-MM-DD HH:MM:SS'
      - ISO-8601 (best effort)
    """
    raw = (build_time_utc_raw or "").strip()
    if not raw:
        # fallback: now
        dt_msk = now_msk_dt()
        m = MONTHS_RU[dt_msk.month - 1]
        return f"{dt_msk.day:02d} {m} {dt_msk.year}, {dt_msk:%H:%M} ĞœĞ¡Ğš"

    s = raw.replace("UTC", "").strip()

    # Try common formats
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%d %H:%M"):
        try:
            dt_utc = datetime.strptime(s, fmt).replace(tzinfo=timezone.utc)
            dt_msk = dt_utc.astimezone(MSK_TZ)
            m = MONTHS_RU[dt_msk.month - 1]
            return f"{dt_msk.day:02d} {m} {dt_msk.year}, {dt_msk:%H:%M} ĞœĞ¡Ğš"
        except Exception:
            pass

    # Try ISO
    try:
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        dt_msk = dt.astimezone(MSK_TZ)
        m = MONTHS_RU[dt_msk.month - 1]
        return f"{dt_msk.day:02d} {m} {dt_msk.year}, {dt_msk:%H:%M} ĞœĞ¡Ğš"
    except Exception:
        return raw


def format_tg_date_time(dt_msk: datetime) -> Tuple[str, str]:
    return dt_msk.strftime("%d.%m.%Y"), dt_msk.strftime("%H:%M:%S ĞœĞ¡Ğš")


# ------------------------- json helpers -------------------------

def load_json(path: Path, default):
    if not path.exists():
        return default
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return default


def dump_json(path: Path, obj) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


# ------------------------- domain set helpers -------------------------

def diff_sets(prev_list: List[str], curr_list: List[str]) -> Tuple[List[str], List[str]]:
    prev = set(prev_list or [])
    curr = set(curr_list or [])
    added = sorted(curr - prev)
    removed = sorted(prev - curr)
    return added, removed


def topn(items: List[str], n: int = 20) -> List[str]:
    return items[:n]


def block_list(items: List[str], indent: str = "") -> str:
    if not items:
        return f"{indent}â€”"
    return "\n".join(f"{indent}{x}" for x in items)


# ------------------------- formatting helpers -------------------------

def short_hash(h: str) -> str:
    h = (h or "").strip()
    if len(h) < 10:
        return h or "â€”"
    return f"{h[:4]}â€¦{h[-4:]}"


def format_change(added: int, removed: int) -> str:
    # note: use "âˆ’" (U+2212) for consistent typography
    return f"+{added} / âˆ’{removed}"


def usage_badge(pct: float) -> str:
    # ğŸŸ¢ <85, ğŸŸ¡ 85â€“96, ğŸ”´ â‰¥96
    if pct >= 96.0:
        return "ğŸ”´"
    if pct >= 85.0:
        return "ğŸŸ¡"
    return "ğŸŸ¢"


def status_text_table(status: str) -> str:
    s = (status or "").strip()
    if s.startswith("OK"):
        return "ğŸŸ¢ ĞĞš"
    if s.startswith("EMPTY"):
        return "ğŸŸ¡ ĞŸĞ£Ğ¡Ğ¢Ğ"
    if s.startswith("FAIL"):
        return "ğŸ”´ ĞĞ¨Ğ˜Ğ‘ĞšĞ"
    return s or "â€”"


def build_run_url() -> Optional[str]:
    server = os.getenv("GITHUB_SERVER_URL", "").strip()
    repo = os.getenv("GITHUB_REPOSITORY", "").strip()
    run_id = os.getenv("GITHUB_RUN_ID", "").strip()
    if server and repo and run_id:
        return f"{server}/{repo}/actions/runs/{run_id}"
    return None


# ------------------------- trend telemetry -------------------------

def append_stats(total: int, itdog: int, v2fly: int, warn_level: str, warn_count: int, error_count: int) -> None:
    data = load_json(STATS_JSON, [])
    if not isinstance(data, list):
        data = []

    rec = {
        "ts_utc": datetime.now(timezone.utc).replace(microsecond=0).isoformat(),
        "total": int(total),
        "itdog": int(itdog),
        "v2fly": int(v2fly),
        "level": warn_level,     # ĞĞš/ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•/ĞĞ¨Ğ˜Ğ‘ĞšĞ
        "warn_count": int(warn_count),
        "error_count": int(error_count),
    }
    data.append(rec)
    data = data[-400:]
    dump_json(STATS_JSON, data)


def last_stats(n: int = 7) -> List[Dict]:
    data = load_json(STATS_JSON, [])
    if not isinstance(data, list):
        return []
    out: List[Dict] = []
    for row in data[-n:]:
        if isinstance(row, dict) and isinstance(row.get("total"), int):
            out.append(row)
    return out


def avg(nums: List[int]) -> Optional[float]:
    if not nums:
        return None
    return sum(nums) / len(nums)


def trend_label(curr: int, avg7: Optional[int], delta_prev: Optional[int]) -> str:
    """
    Produces a short verdict line, aligned to your examples.
    """
    if avg7 is None or delta_prev is None:
        return "â¡ Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾"

    dev = curr - avg7

    # if movement is small, stable
    if abs(delta_prev) <= 5 and abs(dev) <= 30:
        return "â¡ Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾"

    if delta_prev > 0:
        # "Ğ Ğ¾ÑÑ‚ (Ğ²Ñ‹ÑˆĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ã—2)" heuristic:
        # if deviation is at least double the absolute delta and above some floor.
        if dev > 0 and dev >= max(120, abs(delta_prev) * 2):
            return "ğŸ“ˆ Ğ Ğ¾ÑÑ‚ (Ğ²Ñ‹ÑˆĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ã—2)"
        return "ğŸ“ˆ Ğ Ğ¾ÑÑ‚"
    if delta_prev < 0:
        return "ğŸ“‰ ĞŸĞ°Ğ´ĞµĞ½Ğ¸Ğµ"
    return "â¡ Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾"


# ------------------------- severity model -------------------------

@dataclass
class Severity:
    level: str          # ĞĞš/ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•/ĞĞ¨Ğ˜Ğ‘ĞšĞ
    headline: str       # first line
    status_line: str    # second line
    emoji: str          # ğŸŸ¢ğŸŸ¡ğŸ”´


def classify_severity(
    *,
    v2fly_fail: int,
    failed_categories: List[str],
    bad_output_lines: int,
    truncated_count: int,
    usage_pct: float,
    max_lines: int,
    threshold: int,
    empty_categories: List[str],
    warnings: List[str],
) -> Severity:
    # Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ñ ĞĞ¨Ğ˜Ğ‘ĞšĞ
    is_error = (
        v2fly_fail > 0
        or bool(failed_categories)
        or bad_output_lines > 0
        or truncated_count > 0
        or usage_pct >= 96.0
    )

    # Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ñ ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ• (ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°)
    is_warn = (
        (not is_error)
        and (
            bool(empty_categories)
            or bool(warnings)
            or usage_pct >= 85.0
            or (max_lines > 0 and int(round((usage_pct/100.0) * max_lines)) >= threshold)  # compatibility
        )
    )

    if is_error:
        return Severity(
            level="ĞĞ¨Ğ˜Ğ‘ĞšĞ",
            headline="ğŸš¨ Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸",
            status_line="ğŸ”´ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ",
            emoji="ğŸ”´",
        )
    if is_warn:
        return Severity(
            level="ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•",
            headline="âš ï¸ Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° Ñ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
            status_line="ğŸŸ¡ Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
            emoji="ğŸŸ¡",
        )
    return Severity(
        level="ĞĞš",
        headline="ğŸš€ Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°",
        status_line="ğŸŸ¢ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°",
        emoji="ğŸŸ¢",
    )


# ------------------------- main -------------------------

def main() -> int:
    state = load_json(STATE_JSON, {})
    if not isinstance(state, dict):
        raise SystemExit("Bad dist/state.json")

    prev = state.get("prev", {}) if isinstance(state.get("prev"), dict) else {}

    repo = str(state.get("repo", "unknown/unknown"))
    output = str(state.get("output", "dist/inside-kvas.lst"))
    max_lines = int(state.get("max_lines", 3000))
    threshold = int(state.get("near_limit_threshold", 2900))

    itdog_domains = state.get("itdog_domains", []) or []
    v2fly_extras = state.get("v2fly_extras", []) or []
    final_domains = state.get("final_domains", []) or []
    if not isinstance(itdog_domains, list):
        itdog_domains = []
    if not isinstance(v2fly_extras, list):
        v2fly_extras = []
    if not isinstance(final_domains, list):
        final_domains = []

    itdog_set = set(itdog_domains)
    v2fly_set = set(v2fly_extras)
    final_set = set(final_domains)

    itdog_total = len(itdog_set)
    v2fly_total = len(v2fly_set)
    final_total = len(final_set)

    it_added, it_removed = diff_sets(prev.get("itdog_domains", []) or [], itdog_domains)
    v2_added, v2_removed = diff_sets(prev.get("v2fly_extras", []) or [], v2fly_extras)
    f_added, f_removed = diff_sets(prev.get("final_domains", []) or [], final_domains)

    it_change = format_change(len(it_added), len(it_removed))
    v2_change = format_change(len(v2_added), len(v2_removed))
    f_change = format_change(len(f_added), len(f_removed))

    v2fly_ok = int(state.get("v2fly_ok", 0))
    v2fly_fail = int(state.get("v2fly_fail", 0))
    truncated_count = int(state.get("truncated", 0))
    bad_output_lines = int(state.get("bad_output_lines", 0))

    warnings = state.get("warnings", []) or []
    failed_categories = state.get("failed_categories", []) or []
    empty_categories = state.get("empty_categories", []) or []
    if not isinstance(warnings, list):
        warnings = []
    if not isinstance(failed_categories, list):
        failed_categories = []
    if not isinstance(empty_categories, list):
        empty_categories = []

    build_time_utc = str(state.get("build_time_utc", "")).replace(" UTC", "")
    build_time_msk = format_build_time_msk_from_state(build_time_utc)

    sha = short_hash(str(state.get("sha256_final", "")))

    usage_pct = round((final_total / max_lines) * 100, 1) if max_lines else 0.0
    badge = usage_badge(usage_pct)
    remaining = max(0, max_lines - final_total)
    near_limit = final_total >= threshold

    # v2fly per-category
    cats = state.get("v2fly_categories", []) or []
    per_cat = state.get("v2fly_per_category", {}) or {}
    if not isinstance(cats, list):
        cats = []
    if not isinstance(per_cat, dict):
        per_cat = {}
    cats_total = len(cats)
    empty_count = len(empty_categories)

    # classify severity
    sev = classify_severity(
        v2fly_fail=v2fly_fail,
        failed_categories=failed_categories,
        bad_output_lines=bad_output_lines,
        truncated_count=truncated_count,
        usage_pct=usage_pct,
        max_lines=max_lines,
        threshold=threshold,
        empty_categories=empty_categories,
        warnings=warnings,
    )

    # trend stats
    stats7 = last_stats(7)
    totals7 = [int(x["total"]) for x in stats7 if isinstance(x.get("total"), int)]
    avg7 = avg(totals7)
    avg7_int = int(round(avg7)) if avg7 is not None else None

    prev_total = None
    if len(stats7) >= 2 and isinstance(stats7[-2].get("total"), int):
        prev_total = int(stats7[-2]["total"])
    delta_prev = (final_total - prev_total) if prev_total is not None else None

    deviation = (final_total - avg7_int) if avg7_int is not None else None
    trend_verdict = trend_label(final_total, avg7_int, delta_prev) if (avg7_int is not None and delta_prev is not None) else "â¡ Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾"

    # write telemetry AFTER computing delta from previous telemetry entry
    error_count = 0
    warn_count = 0
    if failed_categories:
        error_count += len(failed_categories)
    if v2fly_fail:
        error_count += v2fly_fail
    if bad_output_lines:
        error_count += 1
    if truncated_count:
        error_count += 1
    if usage_pct >= 96.0:
        error_count += 1

    if empty_categories:
        warn_count += len(empty_categories)
    if warnings:
        warn_count += len(warnings)
    if near_limit and usage_pct < 96.0:
        warn_count += 1
    if usage_pct >= 85.0 and usage_pct < 96.0:
        warn_count += 1

    append_stats(final_total, itdog_total, v2fly_total, sev.level, warn_count, error_count)

    # ------------------------- REPORT.MD -------------------------

    critical_lines: List[str] = []
    if failed_categories:
        critical_lines.append(f"ğŸ”´ ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ»Ğ¸ÑÑŒ/Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ğ»Ğ¸ÑÑŒ: {', '.join(failed_categories)}")
    if bad_output_lines > 0:
        critical_lines.append(f"ğŸ”´ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ: {bad_output_lines}")
    if truncated_count > 0:
        critical_lines.append(f"ğŸ”´ ĞĞ±Ñ€ĞµĞ·ĞºĞ° Ğ¿Ğ¾ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñƒ: Ğ”Ğ (Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº: {truncated_count})")
    if usage_pct >= 96.0:
        critical_lines.append(f"ğŸ”´ ĞŸĞ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚: {final_total}/{max_lines} ({usage_pct}%)")

    warn_lines: List[str] = []
    if empty_categories:
        warn_lines.append(f"ğŸŸ¡ ĞŸÑƒÑÑ‚Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ (0 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²): {', '.join(empty_categories)}")
    if warnings:
        # avoid noise: show only first 10
        warn_lines.extend([f"ğŸŸ¡ {w}" for w in warnings[:10]])
        if len(warnings) > 10:
            warn_lines.append(f"ğŸŸ¡ â€¦ĞµÑ‰Ñ‘ {len(warnings) - 10}")

    report: List[str] = []
    report.append("# ğŸ“Š ĞÑ‚Ñ‡Ñ‘Ñ‚ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² KVAS")
    report.append("")
    report.append(f"Ğ¡Ğ±Ğ¾Ñ€ĞºĞ°: {build_time_msk}")
    report.append(f"Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹: {repo}")
    report.append(f"Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ°Ğ¹Ğ»: {output}")
    report.append(f"Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ¾Ğº: {max_lines}")
    report.append("")

    # top severity header
    report.append("ğŸš¦ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞ±Ğ¾Ñ€ĞºĞ¸")
    report.append("")
    report.append(sev.headline)
    report.append(sev.status_line)
    report.append(f"ğŸ§¾ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ² Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ: {bad_output_lines}")
    report.append(f"âœ‚ï¸ ĞĞ±Ñ€ĞµĞ·ĞºĞ° Ğ¿Ğ¾ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñƒ: {'Ğ”Ğ' if truncated_count > 0 else 'ĞĞ•Ğ¢'}")
    report.append("")

    if critical_lines:
        report.append("ğŸ”¥ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹")
        report.extend(critical_lines)
        report.append("")

    report.append("ğŸ“Œ Ğ¡Ğ²Ğ¾Ğ´ĞºĞ°")
    report.append("")
    report.append("itdog")
    report.append("")
    report.append(f"Ğ²ÑĞµĞ³Ğ¾: {itdog_total}")
    report.append(f"Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ: {it_change}")
    report.append("")
    report.append("v2fly (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ extras â€” Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ² itdog)")
    report.append("")
    report.append(f"Ğ²ÑĞµĞ³Ğ¾ extras: {v2fly_total}")
    report.append(f"Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ: {v2_change}")
    report.append("")
    report.append(f"ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: {cats_total} (ğŸŸ¢ ok={v2fly_ok} / ğŸ”´ fail={v2fly_fail} / ğŸŸ¡ Ğ¿ÑƒÑÑ‚Ğ¾={empty_count})")
    report.append("")
    report.append("Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº")
    report.append("")
    report.append(f"Ğ²ÑĞµĞ³Ğ¾: {final_total}")
    report.append(f"Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ: {f_change}")
    report.append(f"Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº: {truncated_count}")
    report.append("")

    report.append("ğŸ“ˆ Ğ›Ğ¸Ğ¼Ğ¸Ñ‚")
    report.append("")
    report.append(f"Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: {final_total} / {max_lines} ({usage_pct}% Ğ·Ğ°Ğ½ÑÑ‚Ğ¾) {badge}")
    report.append(f"Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº: {remaining} ÑÑ‚Ñ€Ğ¾Ğº")
    report.append(f"Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ: {threshold} | Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚: {'Ğ”Ğ' if near_limit else 'ĞĞ•Ğ¢'}")
    report.append("")
    report.append("ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¸:")
    report.append("ğŸŸ¢ Ğ´Ğ¾ 85% â€” Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ | ğŸŸ¡ 85â€“96% â€” Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ | ğŸ”´ â‰¥ 96% â€” ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾")
    report.append("")

    report.append("ğŸ“ˆ Ğ¢Ñ€ĞµĞ½Ğ´")
    report.append("")
    if avg7_int is not None:
        report.append(f"Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ (7): {avg7_int}")
    else:
        report.append("Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ (7): â€”")
    if delta_prev is not None:
        report.append(f"Î” Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹: {delta_prev:+d}")
    else:
        report.append("Î” Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹: â€”")
    if deviation is not None:
        report.append(f"ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ: {deviation:+d}")
    else:
        report.append("ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ: â€”")
    report.append(trend_verdict)
    report.append("")

    report.append("ğŸ”„ Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ itdog (Ñ‚Ğ¾Ğ¿ 20)")
    report.append("â• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾")
    report.append(block_list(topn(it_added, 20)))
    report.append("")
    report.append("â– Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾")
    report.append(block_list(topn(it_removed, 20)))
    report.append("")

    report.append("ğŸ”„ Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ v2fly extras (Ñ‚Ğ¾Ğ¿ 20)")
    report.append("â• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾")
    report.append(block_list(topn(v2_added, 20)))
    report.append("")
    report.append("â– Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾")
    report.append(block_list(topn(v2_removed, 20)))
    report.append("")

    report.append("ğŸ”„ Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¸ÑĞºĞ° (Ñ‚Ğ¾Ğ¿ 20)")
    report.append("â• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾")
    report.append(block_list(topn(f_added, 20)))
    report.append("")
    report.append("â– Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾")
    report.append(block_list(topn(f_removed, 20)))
    report.append("")

    report.append("ğŸ“‚ Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° v2fly Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼")
    report.append("")
    report.append("| ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ | Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² | Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² extras | ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº | ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¸Ğ² | Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ |")
    report.append("|---|---:|---:|---:|---:|---|")
    if cats:
        for cat in cats:
            d = per_cat.get(cat, {}) if isinstance(per_cat.get(cat, {}), dict) else {}
            report.append(
                f"| {cat} | {int(d.get('valid_domains', 0))} | {int(d.get('extras_added', 0))} | "
                f"{int(d.get('invalid_lines', 0))} | {int(d.get('skipped_directives', 0))} | {status_text_table(str(d.get('status', '')))} |"
            )
    else:
        report.append("| â€” | 0 | 0 | 0 | 0 | â€” |")
    report.append("")
    report.append("Ğ›ĞµĞ³ĞµĞ½Ğ´Ğ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ¾Ğ²: ğŸŸ¢ ĞĞš | ğŸŸ¡ ĞŸĞ£Ğ¡Ğ¢Ğ (0 Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²) | ğŸ”´ ĞĞ¨Ğ˜Ğ‘ĞšĞ (ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ/Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³)")
    report.append("")
    report.append("ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ")
    report.append("")
    report.append("- Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² â€” Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° (full:/domain:/Ğ³Ğ¾Ğ»Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹)")
    report.append("- Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² extras â€” Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ğ»Ğ¸ Ğ² Ñ…Ğ²Ğ¾ÑÑ‚ (Ğ½Ğµ Ğ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‚ÑÑ Ñ itdog)")
    report.append("- ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¸Ğ² â€” include:/regexp:/keyword:/etc (Ğ½Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ)")
    report.append("")

    report.append("âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ")
    report.append("")
    if not critical_lines and not warn_lines and not near_limit and truncated_count == 0 and bad_output_lines == 0 and v2fly_fail == 0:
        report.append("âœ… Ğ—Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğ¹ Ğ½ĞµÑ‚")
    else:
        if critical_lines:
            report.append("ğŸ”´ ĞÑˆĞ¸Ğ±ĞºĞ¸")
            report.append("")
            report.extend(critical_lines)
            report.append("")
        if warn_lines or (near_limit and usage_pct < 96.0) or (usage_pct >= 85.0 and usage_pct < 96.0):
            report.append("ğŸŸ¡ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ")
            report.append("")
            if near_limit and usage_pct < 96.0:
                report.append(f"ğŸŸ  ĞŸĞ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ (â‰¥ {threshold})")
            if usage_pct >= 85.0 and usage_pct < 96.0:
                report.append("ğŸŸ¡ Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ° (â‰¥ 85%)")
            if warn_lines:
                report.extend(warn_lines)
            report.append("")

    report.append("ğŸ” Ğ¥ĞµÑˆĞ¸")
    report.append("")
    report.append(f"sha256(final): {sha}")
    report.append("")

    # Diagnostics tail
    intersection = len(itdog_set & v2fly_set)
    reserve = max_lines - final_total
    report.append("ğŸ§ª Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° ÑĞ±Ğ¾Ñ€ĞºĞ¸")
    report.append("")
    report.append(f"Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº itdog: {itdog_total} Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° (ÑƒĞ½Ğ¸Ğº.)")
    report.append(f"v2fly extras: {v2fly_total} Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² (Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ğ¹)")
    report.append(f"Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ itdog âˆ© v2fly: {intersection}")
    report.append(f"Ğ¸Ñ‚Ğ¾Ğ³ Ğ´Ğ¾ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°: {final_total} ÑÑ‚Ñ€Ğ¾Ğº")
    report.append(f"Ğ·Ğ°Ğ¿Ğ°Ñ Ğ´Ğ¾ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ°: {reserve} ÑÑ‚Ñ€Ğ¾Ğº")
    report.append(f"Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒĞµ v2fly: fail={v2fly_fail} ğŸ”´ / empty={empty_count} ğŸŸ¡")
    report.append(f"ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ: {sev.level}")
    report.append("")

    REPORT_OUT.write_text("\n".join(report).rstrip() + "\n", encoding="utf-8")

    # ------------------------- TELEGRAM -------------------------

    dt_msk = now_msk_dt()
    tg_date, tg_time = format_tg_date_time(dt_msk)

    run_url = build_run_url()

    # Build problems list with required icon scheme
    problems: List[str] = []

    # category failures are always red
    for c in failed_categories[:20]:
        problems.append(f"ğŸ”´ {c} â€” fail")

    # empty categories are yellow
    for c in empty_categories[:30]:
        problems.append(f"ğŸŸ¡ {c} â€” Ğ¿ÑƒÑÑ‚Ğ¾")

    # near limit warning / error
    if usage_pct >= 96.0:
        problems.append("ğŸ”´ ĞŸĞ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾)")
    elif usage_pct >= 85.0:
        problems.append("ğŸŸ  ĞŸĞ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚")

    # parse warnings (keep concise)
    for w in warnings[:10]:
        problems.append(f"ğŸŸ¡ {w}")

    if truncated_count > 0:
        problems.append(f"ğŸ”´ ĞĞ±Ñ€ĞµĞ·ĞºĞ° Ğ¿Ğ¾ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñƒ: {truncated_count}")

    if bad_output_lines > 0:
        problems.append(f"ğŸ”´ ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº: {bad_output_lines}")

    # Trend block (as approved)
    trend_lines: List[str] = []
    trend_lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    trend_lines.append("ğŸ“ˆ Ğ¢Ğ Ğ•ĞĞ”")
    trend_lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    if avg7_int is not None:
        trend_lines.append(f"Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ (7): {avg7_int}")
    else:
        trend_lines.append("Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ (7): â€”")
    if delta_prev is not None:
        trend_lines.append(f"Î” Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹: {delta_prev:+d}")
    else:
        trend_lines.append("Î” Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹: â€”")
    if deviation is not None:
        trend_lines.append(f"ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ: {deviation:+d}")
    else:
        trend_lines.append("ĞÑ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ: â€”")
    trend_lines.append(trend_verdict)

    # Message header differs by severity
    tg: List[str] = []
        if sev.level == "ĞĞ¨Ğ˜Ğ‘ĞšĞ":
        tg.append("ğŸš¨ ĞŸÑ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…")
        tg.append(sev.headline)
        tg.append(sev.status_line)
        tg.append("")
        tg.append(f"ğŸ—“ {tg_date}")
        tg.append(f"ğŸ•’ {tg_time}")
        tg.append("")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.append("ğŸ“¦ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.append(f"ğŸ“Š {final_total} / {max_lines} ({usage_pct}%)")
        tg.append(f"ğŸ§® ĞÑÑ‚Ğ°Ñ‚Ğ¾Ğº: {remaining} ÑÑ‚Ñ€Ğ¾Ğº")
        tg.append("")
        tg.extend(trend_lines)
        tg.append("")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.append("âš  ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ«")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.extend(problems or ["â€”"])
        tg.append("")
        tg.append(f"ğŸ” sha256: {sha}")

    elif sev.level == "ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•":
        tg.append("ğŸŸ¡ ĞŸÑ€Ğ¸ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…")
        tg.append(sev.headline)
        tg.append(sev.status_line)
        tg.append("")
        tg.append(f"ğŸ—“ {tg_date}")
        tg.append(f"ğŸ•’ {tg_time}")
        tg.append("")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.append("ğŸ“¦ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.append(f"ğŸ“Š {final_total} / {max_lines} ({usage_pct}%)")
        tg.append(f"ğŸ§® ĞÑÑ‚Ğ°Ñ‚Ğ¾Ğº: {remaining} ÑÑ‚Ñ€Ğ¾Ğº")
        tg.append("")
        tg.extend(trend_lines)
        tg.append("")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.append("âš  ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ«")
        tg.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        tg.extend(problems or ["â€”"])
        tg.append("")
        tg.append(f"ğŸ” sha256: {sha}")

    else:
        tg.append("ğŸŸ¢ ĞŸÑ€Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ")
        tg.append(sev.headline)
        tg.append(sev.status_line)
        tg.append("")
        tg.append(f"ğŸ—“ {tg_date}")
        tg.append(f"ğŸ•’ {tg_time}")
        tg.append("")
        tg.append(f"ğŸ“Š {final_total} / {max_lines} ({usage_pct}%)")
        tg.append("")
        tg.append("ğŸ“ˆ Ğ¢Ğ Ğ•ĞĞ”")
        if avg7_int is not None:
            tg.append(f"Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ (7): {avg7_int}")
        if delta_prev is not None:
            tg.append(f"Î” Ğº Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹: {delta_prev:+d}")
        tg.append(trend_verdict)
        tg.append("")
        tg.append("âœ… Ğ—Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğ¹ Ğ½ĞµÑ‚")
        tg.append("")
        tg.append(f"ğŸ” sha256: {sha}")

    if run_url:
        tg.append("")
        tg.append("ğŸ” Run:")
        tg.append(run_url)

    tg.append("")
    tg.append("ğŸ“ ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚: dist/report.md")

    TG_MESSAGE_OUT.write_text("\n".join(tg).strip() + "\n", encoding="utf-8")

    # Alerts: only ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•/ĞĞ¨Ğ˜Ğ‘ĞšĞ
    if sev.level in ("ĞŸĞ Ğ•Ğ”Ğ£ĞŸĞ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•", "ĞĞ¨Ğ˜Ğ‘ĞšĞ"):
        alert: List[str] = []
        alert.append(f"{sev.emoji} KVAS Domains â€” {sev.level}")
        alert.append(f"ğŸ•’ {tg_date} {tg_time}")
        alert.append("")
        alert.append(f"ğŸ“Š {final_total} / {max_lines} ({usage_pct}%) | Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº: {remaining}")
        if problems:
            alert.append("")
            alert.extend(problems[:25])
        alert.append("")
        alert.append(f"ğŸ” {sha}")
        TG_ALERT_OUT.write_text("\n".join(alert).strip() + "\n", encoding="utf-8")
    else:
        TG_ALERT_OUT.unlink(missing_ok=True)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
